import streamlit as st
import pandas as pd
import re
import time
import json
import google.generativeai as genai
from io import BytesIO

# ==========================================
# è¨­å®šé é¢è³‡è¨Š
# ==========================================
st.set_page_config(
    page_title="å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (AI Auto-Glossing)",
    page_icon="ğŸ§ ",
    layout="wide"
)

# ==========================================
# API Key è¨­å®šå€å¡Š
# ==========================================
apiKey = None

try:
    if "GEMINI_API_KEY" in st.secrets:
        apiKey = st.secrets["GEMINI_API_KEY"]
except FileNotFoundError:
    pass 

if not apiKey:
    with st.sidebar:
        st.markdown("### âš™ï¸ ç³»çµ±è¨­å®š")
        user_api_input = st.text_input("è«‹è¼¸å…¥ Google Gemini API Key", type="password")
        if user_api_input:
            apiKey = user_api_input
        st.caption("â„¹ï¸ è‹¥ç„¡ API Keyï¼Œåƒ…èƒ½é€²è¡ŒåŸºç¤è©æ³•æ‹†è§£ï¼Œç„¡æ³•ä½¿ç”¨ AI è‡ªå‹•è£œå­—ã€‚")
        st.markdown("---")

with st.sidebar:
    if apiKey:
        st.success(f"âœ… API Key å·²è¼‰å…¥")
        st.caption("ğŸš€ æ¨¡å‹ï¼šgemini-2.5-flash (AI è‡ªå‹•è£œå­—é–‹å•Ÿ)")
    else:
        st.warning("âš ï¸ æœªåµæ¸¬åˆ° API Key")

# ==========================================
# 1. æ ¸å¿ƒå­—å…¸åº« (åŸºç¤è³‡æ–™)
# ==========================================
@st.cache_data(persist=True)
def get_dictionary():
    return {
        # --- åŸæœ‰å–®å­— (åƒ…ä¿ç•™éƒ¨åˆ†ç¯„ä¾‹ï¼ŒAI æœƒè™•ç†å‰©ä¸‹çš„) ---
        "tmkuy": {"morph": "t<m>kuy", "gloss": "<ä¸»äº‹ç„¦é»>ç¨®", "meaning": "ç¨®æ¤"},
        "tnkuyan": {"morph": "tnkuy-an", "gloss": "ç”°", "meaning": "ç”°åœ°"},
        "masu": {"morph": "masu", "gloss": "å°ç±³", "meaning": "å°ç±³"},
        "ka": {"morph": "ka", "gloss": "ä¸»æ ¼", "meaning": "(ä¸»æ ¼æ¨™è¨˜)"},
        "ni": {"morph": "ni", "gloss": "é€£æ¥è©", "meaning": "å’Œ/èˆ‡"},
        "o": {"morph": "o", "gloss": "ä¸»é¡Œ", "meaning": "(ä¸»é¡Œæ¨™è¨˜)"},
        "do": {"morph": "do", "gloss": "åŠ©è©", "meaning": "(å¼·èª¿/æ™‚é–“)"},
        "ga": {"morph": "ga", "gloss": "åŠ©è©", "meaning": "(ç‰¹å®š)"},
        "hug": {"morph": "hug", "gloss": "ç–‘å•è©", "meaning": "å—"},
        "da": {"morph": "da", "gloss": "èªå°¾åŠ©è©", "meaning": "äº†"},
        "saw": {"morph": "saw", "gloss": "åƒ", "meaning": "åƒ/å¦‚æ­¤"},
        "kiya": {"morph": "kiya", "gloss": "é‚£", "meaning": "é‚£/æ‰€ä»¥"},
        "kika": {"morph": "kika", "gloss": "é€£æ¥è©", "meaning": "æ‰€ä»¥/å°±æ˜¯"},
        "nasi": {"morph": "nasi", "gloss": "é€£æ¥è©", "meaning": "å¦‚æœ"},
        "ana": {"morph": "ana", "gloss": "ç„¡å®šè©", "meaning": "é›–ç„¶/å³ä½¿"},
        "ida": {"morph": "ida", "gloss": "åŠ©å‹•è©", "meaning": "ä¸€å®š/ä»ç„¶"},
        "ini": {"morph": "ini", "gloss": "å¦å®š", "meaning": "ä¸/æ²’æœ‰"},
        "aji": {"morph": "aji", "gloss": "å¦å®š", "meaning": "ä¸æ˜¯/ä¸è¦"},
        "uxay": {"morph": "uxay", "gloss": "å¦å®š", "meaning": "ä¸æ˜¯"},
        "iya": {"morph": "iya", "gloss": "å¦å®šç¥ˆä½¿", "meaning": "åˆ¥/ä¸è¦"},
        "ungat": {"morph": "ungat", "gloss": "å¦å®šå­˜åœ¨", "meaning": "æ²’æœ‰"},
        "niqan": {"morph": "niqan", "gloss": "å­˜åœ¨", "meaning": "æœ‰"},
        "wada": {"morph": "wada", "gloss": "å®Œæˆè²Œ.åŠ©å‹•", "meaning": "å·²ç¶“/å»"},
        "gisu": {"morph": "gisu", "gloss": "é€²è¡Œè²Œ.åŠ©å‹•", "meaning": "æ­£åœ¨(è¿‘)"},
        "gaga": {"morph": "gaga", "gloss": "é€²è¡Œè²Œ.åŠ©å‹•", "meaning": "æ­£åœ¨(é )/åœ¨é‚£è£¡"},
        "mha": {"morph": "mha", "gloss": "æœªä¾†.åŠ©å‹•", "meaning": "å°‡"},
        "naa": {"morph": "naa", "gloss": "åŠ©å‹•è©", "meaning": "æ‡‰è©²"},
        "ku": {"morph": "ku", "gloss": "1S.ä¸»æ ¼", "meaning": "æˆ‘"},
        "su": {"morph": "su", "gloss": "2S.ä¸»æ ¼/å±¬æ ¼", "meaning": "ä½ /ä½ çš„"},
        "mu": {"morph": "mu", "gloss": "1S.å±¬æ ¼", "meaning": "æˆ‘çš„"},
        "na": {"morph": "na", "gloss": "3S.å±¬æ ¼", "meaning": "ä»–çš„/å°šæœª"},
        "ta": {"morph": "ta", "gloss": "1PL.åŒ…å«.ä¸»æ ¼", "meaning": "æˆ‘å€‘(åŒ…å«)"},
        "nami": {"morph": "nami", "gloss": "1PL.æ’é™¤.ä¸»æ ¼/å±¬æ ¼", "meaning": "æˆ‘å€‘(æ’é™¤)"},
        "namu": {"morph": "namu", "gloss": "2PL.ä¸»æ ¼/å±¬æ ¼", "meaning": "ä½ å€‘"},
        "deha": {"morph": "deha", "gloss": "3PL.ä¸»æ ¼/å±¬æ ¼", "meaning": "ä»–å€‘/äºŒ"},
        "yaku": {"morph": "yaku", "gloss": "1S.ä¸»æ ¼(ç¨ç«‹)", "meaning": "æˆ‘"},
        "isu": {"morph": "isu", "gloss": "2S.ä¸»æ ¼(ç¨ç«‹)", "meaning": "ä½ "},
        "hiya": {"morph": "hiya", "gloss": "3S.ä¸»æ ¼(ç¨ç«‹)", "meaning": "ä»–/å¥¹/é‚£è£¡"},
        "kenan": {"morph": "kenan", "gloss": "1S.æ–œæ ¼", "meaning": "å°æˆ‘/è¢«æˆ‘"},
        "sunan": {"morph": "sunan", "gloss": "2S.æ–œæ ¼", "meaning": "å°ä½ /è¢«ä½ "},
        "menan": {"morph": "menan", "gloss": "1PL.æ’é™¤.æ–œæ ¼", "meaning": "æˆ‘å€‘"},
        "niyi": {"morph": "niyi", "gloss": "æŒ‡ç¤º", "meaning": "é€™/é€™å€‹"}
    }
DICTIONARY = get_dictionary()

# ==========================================
# 2. æ§‹è©è¦å‰‡å¼•æ“ (Rule-Based)
# ==========================================
def analyze_morphology_rule(word):
    # é€™æ˜¯åŸæœ¬çš„è¦å‰‡å¼•æ“ï¼Œç•¶ä½œå‚™ç”¨
    analysis = {"morph": word, "gloss": "???", "meaning": ""}
    
    # ç°¡å–®çš„å‰ç¶´è¦å‰‡ç¯„ä¾‹
    if re.match(r'^m[a-z]+', word) and not word.startswith("ma"):
        if word.startswith("me"):
            return {"morph": f"me-{word[2:]}", "gloss": "ä¸»äº‹ç„¦é»-", "meaning": "(å‹•è©)"}
        elif word.startswith("m"):
            if any(char in "aeiou" for char in word[1:]):
                return {"morph": f"m-{word[1:]}", "gloss": "ä¸»äº‹ç„¦é»-", "meaning": "(å‹•è©)"}
    
    if word.endswith("an"):
        return {"morph": f"{word[:-2]}-an", "gloss": "-è™•æ‰€ç„¦é»", "meaning": "(è™•æ‰€/åè©)"}
    
    return analysis

# ==========================================
# 3. AI æœå‹™æ•´åˆ (ç¿»è­¯ + è‡ªå‹•å­—å…¸)
# ==========================================

# A. å¥å­ç¿»è­¯
@st.cache_data(show_spinner=False)
def call_ai_translation(text, target_lang, gloss_context, api_key):
    if not api_key: return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        if target_lang == 'truku':
            prompt = f"è«‹å°‡ä¸­æ–‡ã€Œ{text}ã€ç¿»è­¯æˆå¤ªé­¯é–£æ—èª(Truku)ã€‚ç›´æ¥çµ¦å‡ºç¿»è­¯å¾Œçš„å¥å­ã€‚"
        else:
            prompt = f"""
            ä½ æ˜¯ä¸€å€‹èªè¨€å­¸å®¶ã€‚è«‹å°‡é€™å¥å¤ªé­¯é–£èª(Truku)ç¿»è­¯æˆä¸­æ–‡ã€‚
            åƒè€ƒè©æ³•ï¼š{gloss_context}
            åŸæ–‡ï¼š{text}
            åªéœ€è¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦è§£é‡‹ã€‚
            """
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception:
        return "(ç¿»è­¯æœå‹™æš«æ™‚ç„¡æ³•ä½¿ç”¨)"

# B. [æ–°åŠŸèƒ½] AI æ‰¹é‡å–®å­—æŸ¥è©¢ (Auto-Glossing)
@st.cache_data(show_spinner=False)
def call_ai_dictionary_batch(words_list, api_key):
    """
    è¼¸å…¥: ['word1', 'word2']
    è¼¸å‡º: JSON å­—ä¸²ï¼ŒåŒ…å«æ¯å€‹å­—çš„ morph, gloss, meaning
    """
    if not api_key or not words_list: return "{}"
    
    try:
        genai.configure(api_key=api_key)
        # è¨­å®šå›æ‡‰ç‚º JSON æ¨¡å¼ (Gemini 2.5/Pro æ”¯æ´)
        model = genai.GenerativeModel('gemini-2.5-flash', generation_config={"response_mime_type": "application/json"})
        
        prompt = f"""
        ä½ æ˜¯ä¸€å€‹å¤ªé­¯é–£èª(Truku)å­—å…¸å°ˆå®¶ã€‚
        è«‹åˆ†æä»¥ä¸‹å–®å­—åˆ—è¡¨ï¼š{words_list}
        
        è«‹å›å‚³ä¸€å€‹ JSON ç‰©ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {{
            "å–®å­—åŸå½¢": {{
                "morph": "æ§‹è©åˆ†æ (ä¾‹å¦‚ m-ekan)", 
                "gloss": "è©æ³•æ¨™è¨˜ (ä¾‹å¦‚ ä¸»äº‹ç„¦é»-åƒ)", 
                "meaning": "ä¸­æ–‡æ„æ€ (ä¾‹å¦‚ åƒ)"
            }}
        }}
        
        æ³¨æ„ï¼š
        1. "morph" æ¬„ä½è«‹æ¨™ç¤ºè©ç¶´åˆ‡åˆ† (å¦‚ m-root, root-an)ã€‚
        2. "gloss" æ¬„ä½è«‹ä½¿ç”¨èªè¨€å­¸ç°¡å¯«æˆ–ä¸­æ–‡æ¨™è¨˜ã€‚
        3. å¦‚æœä¸ç¢ºå®šï¼Œè«‹æ ¹æ“šå¤ªé­¯é–£èªæ§‹è©è¦å‰‡é€²è¡Œæœ€åˆç†çš„æ¨æ¸¬ã€‚
        """
        
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        st.error(f"AI å­—å…¸æŸ¥è©¢å¤±æ•—: {e}")
        return "{}"

# ==========================================
# 4. è¼”åŠ©å‡½å¼ï¼šåˆ‡åˆ†å¥å­
# ==========================================
def split_sentences(text):
    pattern = r'([.?!]+)' 
    parts = re.split(pattern, text)
    sentences = []
    temp_text = ""
    for part in parts:
        if not part: continue 
        if re.match(pattern, part):
            temp_text += part
            sentences.append(temp_text.strip())
            temp_text = "" 
        else:
            temp_text += part
    if temp_text.strip():
        sentences.append(temp_text.strip())
    return sentences

# ==========================================
# ä»‹é¢é‚è¼¯
# ==========================================

st.title("å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (AI Auto-Glossing)")
st.markdown("---")

if "user_input" not in st.session_state:
    st.session_state["user_input"] = ""

# ç¯„ä¾‹æŒ‰éˆ•å›å‘¼
def set_example_text(text):
    st.session_state["user_input"] = text

# å®šç¾©ç¯„ä¾‹ (åŒ…å«å­—å…¸è£¡æ²’æœ‰çš„å­—ï¼Œæ¸¬è©¦ AI èƒ½åŠ›)
ex1_text = "Mtalux bi ka hidaw. Mring kana ka hiyi mu."
ex2_text = "Bhangan ka kari o meiyah ka bgihur paru msa."
ex3_text = "Mlatat su o iya bi snguhi madas bubung."

col1, col2 = st.columns([3, 1])
with col1:
    input_text = st.text_area("è«‹è¼¸å…¥å¥å­ (æ—èªæˆ–ä¸­æ–‡)", height=100, 
                              placeholder="æ”¯æ´å¤šå¥è¼¸å…¥ï¼Œä¾‹å¦‚ï¼šSentence 1. Sentence 2.", 
                              key="user_input")
with col2:
    st.write("ç¯„ä¾‹ (æ¸¬è©¦ AI è£œå­—)ï¼š")
    st.button("ç¯„ä¾‹ 1", on_click=set_example_text, args=(ex1_text,))
    st.button("ç¯„ä¾‹ 2", on_click=set_example_text, args=(ex2_text,))
    st.button("ç¯„ä¾‹ 3", on_click=set_example_text, args=(ex3_text,))

# ç”¨ä¾†æ”¶é›†æœ¬æ¬¡ AI æŸ¥åˆ°çš„æ–°å­—ï¼Œä¾›å´é‚Šæ¬„ä¸‹è¼‰
if "ai_generated_dict" not in st.session_state:
    st.session_state["ai_generated_dict"] = {}

# åˆ†ææŒ‰éˆ•
if st.button("é–‹å§‹åˆ†æ", type="primary"):
    input_content = st.session_state["user_input"]
    
    if not input_content:
        st.warning("è«‹è¼¸å…¥æ–‡å­—")
    else:
        # 1. é è™•ç†ï¼šæ‰¾å‡ºæ•´æ®µæ–‡ç« ä¸­ã€Œå­—å…¸æ²’æœ‰çš„å­—ã€
        all_words_in_text = re.findall(r"\b[a-zA-Z]+\b", input_content.lower())
        unknown_words = []
        for w in all_words_in_text:
            if w not in DICTIONARY and w not in st.session_state["ai_generated_dict"]:
                unknown_words.append(w)
        
        # 2. å¦‚æœæœ‰ç”Ÿå­—ï¼Œå‘¼å« AI æ‰¹é‡æŸ¥è©¢ (Batch AI Lookup)
        if unknown_words and apiKey:
            with st.status("ğŸ” ç™¼ç¾é™Œç”Ÿå–®å­—ï¼Œæ­£åœ¨è©¢å• AI å­—å…¸...", expanded=True) as status:
                st.write(f"æ­£åœ¨æŸ¥è©¢ï¼š{', '.join(set(unknown_words))}")
                
                # å‘¼å« AI
                ai_dict_json = call_ai_dictionary_batch(list(set(unknown_words)), apiKey)
                
                try:
                    # è§£æ JSON ä¸¦å­˜å…¥ session state
                    new_entries = json.loads(ai_dict_json)
                    st.session_state["ai_generated_dict"].update(new_entries)
                    status.update(label="âœ… AI å­—å…¸æ›´æ–°å®Œæˆï¼", state="complete", expanded=False)
                except json.JSONDecodeError:
                    st.error("AI å›å‚³æ ¼å¼éŒ¯èª¤ï¼Œå°‡ä½¿ç”¨è¦å‰‡å¼•æ“å‚™æ´ã€‚")

        # 3. é–‹å§‹é€å¥åˆ†æ
        sentence_list = split_sentences(input_content)
        all_csv_data = [["Line", "Content"]]

        for idx, single_sentence in enumerate(sentence_list):
            if len(sentence_list) > 1:
                st.markdown(f"#### å¥å­ {idx + 1}")

            with st.spinner(f"åˆ†æä¸­... ({idx+1}/{len(sentence_list)})"):
                # åˆ¤æ–·èªè¨€
                is_chinese = any("\u4e00" <= char <= "\u9fff" for char in single_sentence)
                source_text = single_sentence
                translation_text = ""

                # ä¸­æ–‡ -> æ—èª
                if is_chinese:
                    if not apiKey:
                        st.error("éœ€è¦ API Keyã€‚")
                        st.stop()
                    source_text = call_ai_translation(source_text, 'truku', "", apiKey)
                    translation_text = single_sentence

                # æ§‹è©åˆ†æ (çµåˆ å…§å»ºå­—å…¸ + AI å­—å…¸ + è¦å‰‡)
                clean_text = re.sub(r'[.,?!;:ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]', '', source_text).lower()
                raw_words = source_text.split()
                analyzed_words = []

                for word in raw_words:
                    clean_word = re.sub(r'[.,?!;:ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]', '', word).lower()
                    
                    # å„ªå…ˆåº 1: å…§å»ºå­—å…¸
                    if clean_word in DICTIONARY:
                        d = DICTIONARY[clean_word]
                        analyzed_words.append({"original": word, "morph": d["morph"], "gloss": d["gloss"], "meaning": d["meaning"]})
                    
                    # å„ªå…ˆåº 2: AI å‰›å‰›ç”Ÿæˆçš„å­—å…¸ (Session State)
                    elif clean_word in st.session_state["ai_generated_dict"]:
                        d = st.session_state["ai_generated_dict"][clean_word]
                        analyzed_words.append({"original": word, "morph": d.get("morph", clean_word), "gloss": d.get("gloss", "AI"), "meaning": d.get("meaning", "?")})
                    
                    # å„ªå…ˆåº 3: è¦å‰‡å¼•æ“ (æœ€å¾Œæ‰‹æ®µ)
                    else:
                        guess = analyze_morphology_rule(clean_word)
                        analyzed_words.append({"original": word, "morph": guess["morph"], "gloss": guess["gloss"], "meaning": guess["meaning"]})

                # æ—èª -> ä¸­æ–‡
                if not is_chinese:
                    gloss_context = " ".join([f"{w['original']}({w['gloss']}/{w['meaning']})" for w in analyzed_words])
                    if apiKey:
                        translation_text = call_ai_translation(source_text, 'chinese', gloss_context, apiKey)
                    else:
                        translation_text = "(æœªè¨­å®š API Key)"

                # é¡¯ç¤ºçµæœ
                html_output = f"""
                <div style="font-family: monospace; font-size: 16px; line-height: 1.8; background-color: #f8f9fa; color: #1f2937; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                    <div style="margin-bottom: 8px;"><span style="color: #e11d48; font-weight: bold;">â—</span> {' '.join([w['original'] for w in analyzed_words])}</div>
                    <div style="margin-bottom: 8px;"><span style="color: #2563eb; font-weight: bold;">â—</span> {' '.join([w['morph'] for w in analyzed_words])}</div>
                    <div style="margin-bottom: 8px;"><span style="color: #059669; font-weight: bold;">â—</span> {' '.join([w['gloss'] for w in analyzed_words])}</div>
                    <div style="margin-top: 12px; font-weight: bold; border-top: 1px solid #e5e7eb; padding-top: 8px;"><span style="color: #d97706;">â—</span> {translation_text}</div>
                </div>
                """
                st.markdown(html_output, unsafe_allow_html=True)

                # æ”¶é›† CSV
                all_csv_data.append([f"Sentence {idx+1} - Line 1", ' '.join([w['original'] for w in analyzed_words])])
                all_csv_data.append([f"Sentence {idx+1} - Line 2", ' '.join([w['morph'] for w in analyzed_words])])
                all_csv_data.append([f"Sentence {idx+1} - Line 3", ' '.join([w['gloss'] for w in analyzed_words])])
                all_csv_data.append([f"Sentence {idx+1} - Line 4", translation_text])
                all_csv_data.append(["---", "---"])

        # åŒ¯å‡ºåˆ†æçµæœ
        csv = pd.DataFrame(all_csv_data).to_csv(index=False, header=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ åŒ¯å‡ºåˆ†æçµæœ (CSV)", csv, 'truku_analysis.csv', 'text/csv')

# ==========================================
# å´é‚Šæ¬„ï¼šæ–°è©å½™ç®¡ç† (æ‰¹æ¬¡å¢åŠ çš„ç§˜å¯†æ­¦å™¨)
# ==========================================
if st.session_state["ai_generated_dict"]:
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ§  AI è‡ªå‹•å­¸ç¿’çš„æ–°è©")
    st.sidebar.caption("é€™äº›æ˜¯ AI å‰›å‰›è‡ªå‹•è£œå……çš„å–®å­—ï¼Œæ‚¨å¯ä»¥ä¸‹è¼‰å¾ŒåŠ å›åŸå§‹ç¢¼ä¸­ã€‚")
    
    # è½‰æˆ DataFrame é¡¯ç¤º
    new_words_data = []
    for k, v in st.session_state["ai_generated_dict"].items():
        new_words_data.append({"å–®å­—": k, "æ§‹è©": v['morph'], "è©æ³•": v['gloss'], "è©ç¾©": v['meaning']})
    
    df_new = pd.DataFrame(new_words_data)
    st.sidebar.dataframe(df_new, hide_index=True)
    
    # æä¾›ä¸‹è¼‰æ–°è©å…¸æ ¼å¼
    # æ ¼å¼åŒ–æˆ Python DICTIONARY çš„å­—ä¸²æ ¼å¼ï¼Œæ–¹ä¾¿è¤‡è£½è²¼ä¸Š
    dict_str = ""
    for k, v in st.session_state["ai_generated_dict"].items():
        dict_str += f'    "{k}": {{"morph": "{v["morph"]}", "gloss": "{v["gloss"]}", "meaning": "{v["meaning"]}"}},\n'
    
    st.sidebar.download_button(
        label="ğŸ“¥ ä¸‹è¼‰æ–°è© (Python æ ¼å¼)",
        data=dict_str,
        file_name="new_dictionary_entries.txt",
        mime="text/plain"
    )

st.markdown("---")
st.caption("è³‡æ–™ä¾†æºåƒè€ƒï¼šã€Šå¤ªé­¯é–£èªèªæ³•æ¦‚è«–ã€‹ + Gemini AI è‡ªå‹•è£œå­— | è¨­è¨ˆç”¨é€”ï¼šæ—èªæ•™å­¸èˆ‡èªæ–™ä¿å­˜")
