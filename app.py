import streamlit as st
import pandas as pd
import re
import time
import json
import os
import google.generativeai as genai
from io import BytesIO

# ==========================================
# è¨­å®šé é¢è³‡è¨Š
# ==========================================
st.set_page_config(
    page_title="å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (AI Pro)",
    page_icon="ğŸ“–",
    layout="wide"
)

# ==========================================
# API Key è¨­å®šå€å¡Š
# ==========================================
apiKey = None

# å˜—è©¦å¾ secrets è®€å–ï¼Œè‹¥ç„¡å‰‡é¡¯ç¤ºå´é‚Šæ¬„è¼¸å…¥
try:
    if "GEMINI_API_KEY" in st.secrets:
        apiKey = st.secrets["GEMINI_API_KEY"]
except FileNotFoundError:
    pass 

if not apiKey:
    with st.sidebar:
        st.markdown("### âš™ï¸ ç³»çµ±è¨­å®š")
        user_api_input = st.text_input("è«‹è¼¸å…¥ Google Gemini API Key", type="password")
        if user_api_input:
            apiKey = user_api_input
        st.caption("â„¹ï¸ è‹¥ç„¡ API Keyï¼Œåƒ…èƒ½é€²è¡Œè©æ³•æ‹†è§£ï¼Œç„¡æ³•ä½¿ç”¨æ•´å¥ç¿»è­¯ã€‚")
        st.markdown("---")

with st.sidebar:
    if apiKey:
        st.success(f"âœ… API Key å·²è¼‰å…¥")
    else:
        st.warning("âš ï¸ æœªåµæ¸¬åˆ° API Key")

# ==========================================
# 1. æ ¸å¿ƒå­—å…¸åº« (å‡ç´šç‰ˆï¼šè®€å– Excel)
# ==========================================
@st.cache_data(persist=True)
def load_excel_dictionary(filepath):
    """
    è®€å– truku_dictionary_final.xlsx ä¸¦è½‰æ›ç‚º App èƒ½ç”¨çš„æ ¼å¼
    """
    dictionary = {}
    
    # é è¨­çš„ä¸€äº›åŸºç¤å–®å­— (é˜²å‘†ç”¨)
    base_dict = {
        "ka": {"morph": "ka", "gloss": "ä¸»æ ¼", "meaning": "(ä¸»æ ¼æ¨™è¨˜)"},
        "ni": {"morph": "ni", "gloss": "é€£æ¥è©", "meaning": "å’Œ/èˆ‡"},
    }
    dictionary.update(base_dict)

    if not os.path.exists(filepath):
        st.error(f"âŒ æ‰¾ä¸åˆ°è¾­å…¸æª”ï¼š{filepath}ã€‚è«‹ç¢ºä¿æª”æ¡ˆå·²ä¸Šå‚³ã€‚")
        return dictionary

    try:
        df = pd.read_excel(filepath)
        # è‡ªå‹•åµæ¸¬æ¬„ä½ (é˜²å‘†)
        word_col = next((c for c in df.columns if 'Word' in c or 'word' in c), None)
        gloss_col = next((c for c in df.columns if 'Gloss' in c or 'gloss' in c), None)
        
        if word_col and gloss_col:
            for _, row in df.iterrows():
                word = str(row[word_col]).strip().lower()
                gloss = str(row[gloss_col]).strip()
                
                # å¦‚æœ Gloss æ˜¯ç©ºç™½æˆ– ???ï¼Œå°±ç•¥éæˆ–æ¨™è¨˜
                if gloss == "nan" or gloss == "???" or not gloss:
                    continue

                # å°‡ Excel è³‡æ–™å­˜å…¥å­—å…¸
                # æ³¨æ„ï¼šå› ç‚ºè¾­å…¸æª”åªæœ‰ Word å’Œ Glossï¼Œæˆ‘å€‘å…ˆæŠŠ Gloss åŒæ™‚ç•¶ä½œ Meaning
                dictionary[word] = {
                    "morph": word,       # è¾­å…¸æª”ç›®å‰æ²’æœ‰æ§‹è©æ‹†è§£ï¼Œæš«æ™‚ç”¨åŸè©
                    "gloss": gloss,
                    "meaning": gloss     # æš«æ™‚ç”¨ gloss ç•¶ä½œ meaning
                }
        else:
            st.error("âŒ è¾­å…¸æª”æ¬„ä½åç¨±ä¸ç¬¦ (éœ€è¦ Word å’Œ Gloss)")
            
    except Exception as e:
        st.error(f"è®€å–è¾­å…¸å¤±æ•—: {e}")

    return dictionary

# --- é€™è£¡è¨­å®šæ‚¨çš„è¾­å…¸æª”å ---
DICT_FILE = 'truku_dictionary_final.xlsx'
DICTIONARY = load_excel_dictionary(DICT_FILE)

# é¡¯ç¤ºè¼‰å…¥ç‹€æ³
with st.sidebar:
    st.info(f"ğŸ“š å·²è¼‰å…¥è¾­å…¸ï¼š{len(DICTIONARY)} è©æ¢")

# ==========================================
# 2. æ§‹è©è¦å‰‡å¼•æ“ (æœªæŸ¥åˆ°å–®å­—æ™‚çš„çŒœæ¸¬)
# ==========================================
def analyze_morphology(word):
    analysis = {"morph": word, "gloss": "???", "meaning": ""}
    
    # å¸¸ç”¨å‰ç¶´è¦å‰‡
    if re.match(r'^m[a-z]+', word) and not word.startswith("ma"):
        if word.startswith("me"):
            root = word[2:]
            return {"morph": f"me-{root}", "gloss": "ä¸»äº‹ç„¦é»-", "meaning": "(å‹•è©)"}
        elif word.startswith("m"):
            root = word[1:]
            if any(char in "aeiou" for char in root):
                return {"morph": f"m-{root}", "gloss": "ä¸»äº‹ç„¦é»-", "meaning": "(å‹•è©)"}
    
    # ä¸­ç¶´ -m-
    if len(word) > 3 and word[1] == 'm' and word[2] in "aeiou":
         root = word[0] + word[2:]
         return {"morph": f"{word[0]}<m>{word[2:]}", "gloss": "<ä¸»äº‹ç„¦é»>", "meaning": "(å‹•è©)"}

    # ä¸­ç¶´ -n- (å®Œæˆè²Œ)
    if len(word) > 3 and word[1] == 'n' and word[2] in "aeiou":
         root = word[0] + word[2:]
         return {"morph": f"{word[0]}<n>{word[2:]}", "gloss": "<å®Œæˆè²Œ>", "meaning": "(å‹•è©)"}
    
    # å¸¸ç”¨å¾Œç¶´
    if word.endswith("un"):
        root = word[:-2]
        return {"morph": f"{root}-un", "gloss": "-å—äº‹ç„¦é»", "meaning": "(è¢«å‹•/æœªä¾†)"}
    if word.endswith("an"):
        root = word[:-2]
        return {"morph": f"{root}-an", "gloss": "-è™•æ‰€ç„¦é»", "meaning": "(è™•æ‰€/éå»)"}
    if word.endswith("i"):
        root = word[:-1]
        return {"morph": f"{root}-i", "gloss": "-ç¥ˆä½¿", "meaning": "(å‘½ä»¤)"}

    return analysis

# ==========================================
# 3. AI ç¿»è­¯ API
# ==========================================
@st.cache_data(show_spinner=False)
def call_ai_translation(text, target_lang, gloss_context, api_key):
    if not api_key:
        return None

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')

        if target_lang == 'truku':
            prompt = f"è«‹å°‡ä»¥ä¸‹ä¸­æ–‡å¥å­ç¿»è­¯æˆå¤ªé­¯é–£æ—èª(Truku)ã€‚ç›´æ¥çµ¦å‡ºç¿»è­¯å¾Œçš„æ—èªå¥å­å³å¯ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡‹æˆ–æ‹¼éŸ³ã€‚\nå¥å­ï¼š{text}"
        else:
            prompt = f"""
            ä½ æ˜¯ä¸€å€‹ç²¾é€šå¤ªé­¯é–£èª(Truku)èˆ‡ä¸­æ–‡çš„èªè¨€å­¸å®¶ã€‚è«‹é€²è¡Œä»¥ä¸‹ç¿»è­¯ä»»å‹™ï¼š
            1. **çµæ§‹å°æ‡‰**ï¼šåƒè€ƒæä¾›çš„ [è©æ³•åˆ†æ] (Gloss)ã€‚
            2. **ç›´è­¯**ï¼šå…ˆé€²è¡Œè©å°è©ç›´è­¯ã€‚
            3. **èªæ„å„ªåŒ–**ï¼šå°‡ç›´è­¯çµæœèª¿æ•´ç‚ºé€šé †ä¸­æ–‡ã€‚

            åŸæ–‡ï¼š{text}
            è©æ³•åˆ†æåƒè€ƒï¼š{gloss_context}

            è«‹ç›´æ¥è¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡‹ã€‚
            """
        
        response = model.generate_content(prompt)
        return response.text.strip()
    
    except Exception as e:
        return f"ERROR: {str(e)}"

# ==========================================
# 4. è¼”åŠ©å‡½å¼ï¼šåˆ‡åˆ†å¥å­
# ==========================================
def split_sentences(text):
    pattern = r'([.?!]+)' 
    parts = re.split(pattern, text)
    sentences = []
    temp_text = ""
    for part in parts:
        if not part: continue
        if re.match(pattern, part):
            temp_text += part
            sentences.append(temp_text.strip())
            temp_text = ""
        else:
            temp_text += part
    if temp_text.strip():
        sentences.append(temp_text.strip())
    return sentences

# ==========================================
# ä»‹é¢é‚è¼¯
# ==========================================

st.title("å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (Pro)")
st.markdown("---")

if "user_input" not in st.session_state:
    st.session_state["user_input"] = ""

def set_example_text(text):
    st.session_state["user_input"] = text

col1, col2 = st.columns([3, 1])

with col1:
    input_text = st.text_area("è«‹è¼¸å…¥å¥å­ (æ—èªæˆ–ä¸­æ–‡)", height=100, 
                              placeholder="æ”¯æ´å¤šå¥è¼¸å…¥ï¼Œä¾‹å¦‚ï¼šMtalux bi ka hidaw.", 
                              key="user_input")

with col2:
    st.write("æ¸¬è©¦ç¯„ä¾‹ï¼š")
    st.button("ç¯„ä¾‹ 1: å¤©æ°£", on_click=set_example_text, args=("Mtalux bi ka hidaw.",))
    st.button("ç¯„ä¾‹ 2: å°è©±", on_click=set_example_text, args=("Ima hangan na ka qbsuran su kuyuh?",))

# åˆ†ææŒ‰éˆ•
if st.button("é–‹å§‹åˆ†æ", type="primary"):
    input_content = st.session_state["user_input"]
    
    if not input_content:
        st.warning("è«‹è¼¸å…¥æ–‡å­—")
    else:
        # --- åˆ‡åˆ†å¥å­ ---
        sentence_list = split_sentences(input_content)
        
        # æº–å‚™ CSV åŒ¯å‡ºè³‡æ–™
        all_csv_data = []
        all_csv_data.append(["Line", "Content"])

        # é€å¥è™•ç†
        for idx, single_sentence in enumerate(sentence_list):
            
            if len(sentence_list) > 1:
                st.markdown(f"#### å¥å­ {idx + 1}")

            with st.spinner(f"åˆ†æä¸­... ({idx+1}/{len(sentence_list)})"):
                # 1. åˆ¤æ–·èªè¨€æ¨¡å¼
                is_chinese = any("\u4e00" <= char <= "\u9fff" for char in single_sentence)
                
                source_text = single_sentence
                translation_text = ""

                # 2. ä¸­æ–‡ -> æ—èª (AI)
                if is_chinese:
                    if not apiKey:
                        st.error("éœ€è¦ API Key æ‰èƒ½ç¿»è­¯ä¸­æ–‡ã€‚")
                        st.stop()
                    
                    ai_result = call_ai_translation(source_text, 'truku', "", apiKey)
                    if ai_result and not ai_result.startswith("ERROR:"):
                        translation_text = source_text
                        source_text = ai_result
                    else:
                        translation_text = "(ç¿»è­¯å¤±æ•—)"

                # 3. æ§‹è©åˆ†æ (æ ¸å¿ƒå‡ç´šï¼šæŸ¥ Excel å­—å…¸)
                clean_text = re.sub(r'[.,?!;:ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]', '', source_text).lower()
                raw_words = source_text.split()
                
                analyzed_words = []
                for word in raw_words:
                    # å»é™¤æ¨™é»ä¸¦è½‰å°å¯«ä»¥é€²è¡ŒæŸ¥è¡¨
                    clean_word = re.sub(r'[.,?!;:ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼š]', '', word).lower()
                    
                    # å„ªå…ˆæŸ¥è¡¨
                    if clean_word in DICTIONARY:
                        data = DICTIONARY[clean_word]
                        # å› ç‚º Excel å­—å…¸æ²’ç´€éŒ„ morph æ‹†è§£ï¼Œé€™é‚Š morph æš«æ™‚é¡¯ç¤ºåŸè©
                        # å¦‚æœæ‚¨æœªä¾†æœ‰æ›´ç²¾ç´°çš„è³‡æ–™ï¼Œå¯ä»¥å†èª¿æ•´
                        analyzed_words.append({"original": word, "morph": word, "gloss": data["gloss"], "meaning": data["meaning"]})
                    else:
                        # æŸ¥ä¸åˆ°å‰‡ä½¿ç”¨è¦å‰‡çŒœæ¸¬
                        guess = analyze_morphology(clean_word)
                        analyzed_words.append({"original": word, "morph": guess["morph"], "gloss": guess["gloss"], "meaning": guess["meaning"]})

                # 4. æ—èª -> ä¸­æ–‡ (AI)
                if not is_chinese:
                    # çµ„åˆ Gloss çµ¦ AI åƒè€ƒï¼Œè®“ç¿»è­¯æ›´æº–
                    gloss_context = " ".join([f"{w['original']}({w['gloss']})" for w in analyzed_words if w['gloss'] != "???"])
                    
                    if apiKey:
                        ai_result = call_ai_translation(source_text, 'chinese', gloss_context, apiKey)
                        if ai_result and not ai_result.startswith("ERROR:"):
                             translation_text = ai_result
                        else:
                             translation_text = "(ç¿»è­¯å¤±æ•—)"
                    else:
                        translation_text = "(æœªè¨­å®š API Key)"

                # 5. é¡¯ç¤ºçµæœ (å››è¡Œå°ç…§)
                html_output = f"""
                <div style="font-family: monospace; font-size: 16px; line-height: 1.8; background-color: #f8f9fa; color: #1f2937; padding: 20px; border-radius: 10px; margin-bottom: 20px;">
                    <div style="margin-bottom: 8px;"><span style="color: #e11d48; font-weight: bold;">word:</span> {' '.join([w['original'] for w in analyzed_words])}</div>
                    <div style="margin-bottom: 8px;"><span style="color: #2563eb; font-weight: bold;">gloss:</span> {' '.join([w['gloss'] for w in analyzed_words])}</div>
                    <div style="margin-top: 12px; font-weight: bold; border-top: 1px solid #e5e7eb; padding-top: 8px;"><span style="color: #d97706;">Trans:</span> {translation_text}</div>
                </div>
                """
                st.markdown(html_output, unsafe_allow_html=True)

                # æ”¶é›† CSV
                all_csv_data.append([f"S{idx+1}-L1", ' '.join([w['original'] for w in analyzed_words])])
                all_csv_data.append([f"S{idx+1}-L2", ' '.join([w['gloss'] for w in analyzed_words])])
                all_csv_data.append([f"S{idx+1}-L3", translation_text])
                all_csv_data.append(["---", "---"])

        # 6. åŒ¯å‡ºåŠŸèƒ½
        df_export = pd.DataFrame(all_csv_data)
        csv = df_export.to_csv(index=False, header=False).encode('utf-8-sig')
        
        st.download_button(
            label="åŒ¯å‡º Excel (CSV)",
            data=csv,
            file_name='truku_analysis_result.csv',
            mime='text/csv',
        )

st.markdown("---")
st.caption("è³‡æ–™ä¾†æºï¼šè‡ªå‹•åŒ– Excel è¾­å…¸åº« | è¨­è¨ˆç”¨é€”ï¼šæ—èªæ•™å­¸èˆ‡èªæ–™ä¿å­˜")
