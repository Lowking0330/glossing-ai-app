import streamlit as st
import pandas as pd
import re
import time
import json
import google.generativeai as genai
from io import BytesIO

# ==========================================
# è¨­å®šé é¢è³‡è¨Š
# ==========================================
st.set_page_config(
    page_title="å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (AI Context-Aware)",
    page_icon="ğŸ¯",
    layout="wide"
)

# ==========================================
# API Key è¨­å®šå€å¡Š
# ==========================================
apiKey = None

try:
    if "GEMINI_API_KEY" in st.secrets:
        apiKey = st.secrets["GEMINI_API_KEY"]
except FileNotFoundError:
    pass 

if not apiKey:
    with st.sidebar:
        st.markdown("### âš™ï¸ ç³»çµ±è¨­å®š")
        user_api_input = st.text_input("è«‹è¼¸å…¥ Google Gemini API Key", type="password")
        if user_api_input:
            apiKey = user_api_input
        st.caption("â„¹ï¸ æ­¤ç‰ˆæœ¬æ¥µåº¦ä¾è³´ AI é€²è¡Œèªæ„åˆ¤è®€ï¼Œè«‹å‹™å¿…è¼¸å…¥ API Keyã€‚")
        st.markdown("---")

with st.sidebar:
    if apiKey:
        st.success(f"âœ… API Key å·²è¼‰å…¥")
        st.caption("ğŸš€ æ¨¡å‹ï¼šgemini-2.5-flash (èªå¢ƒç²¾æº–æ¨¡å¼)")
    else:
        st.warning("âš ï¸ æœªåµæ¸¬åˆ° API Key")

# ==========================================
# 1. æ ¸å¿ƒå­—å…¸åº« (ä½œç‚ºå‚™ç”¨è³‡æ–™åº«)
# ==========================================
# é›–ç„¶æˆ‘å€‘ä¾è³´ AIï¼Œä½†ä¿ç•™å­—å…¸å¯ä»¥åœ¨ AI å¤±æ•—æ™‚ä½œç‚ºä¿éšª
@st.cache_data(persist=True)
def get_dictionary():
    return {
        "ka": {"morph": "ka", "gloss": "ä¸»æ ¼", "meaning": "(ä¸»æ ¼æ¨™è¨˜)"},
        "ni": {"morph": "ni", "gloss": "é€£æ¥è©", "meaning": "å’Œ/èˆ‡"},
        "o": {"morph": "o", "gloss": "ä¸»é¡Œ", "meaning": "(ä¸»é¡Œæ¨™è¨˜)"},
        "do": {"morph": "do", "gloss": "åŠ©è©", "meaning": "(å¼·èª¿/æ™‚é–“)"},
        "ga": {"morph": "ga", "gloss": "åŠ©è©", "meaning": "(ç‰¹å®š)"},
        # ... (æ‚¨å¯ä»¥ç¹¼çºŒä¿ç•™èˆŠæœ‰çš„å­—å…¸ï¼Œé€™è£¡çœç•¥ä»¥ç¯€çœç¯‡å¹…)
    }
DICTIONARY = get_dictionary()

# ==========================================
# 2. AI æ ¸å¿ƒåŠŸèƒ½ï¼šæ•´å¥èªå¢ƒåˆ†æ (Context-Aware Glossing)
# ==========================================
@st.cache_data(show_spinner=False)
def call_ai_sentence_analysis(sentence, api_key):
    """
    è¼¸å…¥ï¼šæ•´å¥å¤ªé­¯é–£èª (ä¾‹å¦‚ "Mha ku qmita tnkuyan mu.")
    è¼¸å‡ºï¼šä¸€å€‹åŒ…å«æ¯å€‹å­—è©³ç´°åˆ†æçš„ JSON List
    """
    if not api_key: return None

    try:
        genai.configure(api_key=api_key)
        # è¨­å®šå›å‚³æ ¼å¼ç‚º JSONï¼Œç¢ºä¿ç¨‹å¼èƒ½ç²¾æº–è®€å–
        model = genai.GenerativeModel('gemini-2.5-flash', generation_config={"response_mime_type": "application/json"})

        prompt = f"""
        ä½ æ˜¯ä¸€ä½ç²¾é€šå¤ªé­¯é–£èª(Truku/Seediq)çš„èªè¨€å­¸å®¶ã€‚
        è«‹å°ä»¥ä¸‹å¥å­é€²è¡Œã€Œé€å­—æ§‹è©æ¨™è¨» (Interlinear Glossing)ã€ã€‚
        
        å¥å­ï¼š"{sentence}"

        è«‹ä»”ç´°åˆ†æé€™å¥è©±çš„èªæ³•çµæ§‹èˆ‡ä¸Šä¸‹æ–‡ï¼Œç„¶å¾Œå›å‚³ä¸€å€‹ JSON é™£åˆ— (List)ã€‚
        é™£åˆ—ä¸­çš„æ¯ä¸€å€‹ç‰©ä»¶ä»£è¡¨å¥å­è£¡çš„ä¸€å€‹å–®å­—(Word)ï¼Œå¿…é ˆåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        
        1. "original": åŸå­— (åŒ…å«æ¨™é»ç¬¦è™Ÿè«‹ç¨ç«‹åˆ‡åˆ†)
        2. "morph": æ§‹è©åˆ†æ (ä¾‹å¦‚ m-ekan, s<m>ruwa, root-an)ã€‚å¦‚æœæ˜¯å–®ç´”åè©å‰‡ç¶­æŒåŸæ¨£ã€‚
        3. "gloss": èªæ³•æ¨™è¨˜ (è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œå¦‚ï¼šä¸»äº‹ç„¦é»ã€ä¸»æ ¼ã€å±¬æ ¼ã€æœªä¾†ã€å®Œæˆ)ã€‚
        4. "meaning": **é‡é»**ï¼šè«‹æä¾›è©²å­—åœ¨ã€Œé€™å€‹å¥å­ä¸­ã€çš„ç¢ºåˆ‡ä¸­æ–‡æ„æ€ã€‚ä¸è¦çµ¦å­—å…¸åŸæ„ï¼Œè¦çµ¦ä¸Šä¸‹æ–‡æ„ã€‚

        ç¯„ä¾‹è¼¸å‡ºæ ¼å¼ï¼š
        [
            {{"original": "Mha", "morph": "mha", "gloss": "æœªä¾†", "meaning": "å°‡è¦"}},
            {{"original": "ku", "morph": "ku", "gloss": "1S.ä¸»æ ¼", "meaning": "æˆ‘"}},
            {{"original": "qmita", "morph": "q<m>ita", "gloss": "ä¸»äº‹ç„¦é»-çœ‹", "meaning": "çœ‹"}},
            ...
        ]
        
        è«‹ç¢ºä¿å›å‚³çš„æ˜¯æ¨™æº– JSON æ ¼å¼ï¼Œä¸è¦æœ‰ Markdown æ¨™è¨˜ã€‚
        """
        
        response = model.generate_content(prompt)
        return json.loads(response.text)
    
    except Exception as e:
        return {"error": str(e)}

# ç¿»è­¯æ•´å¥ (è¼”åŠ©ç”¨)
@st.cache_data(show_spinner=False)
def call_ai_translation(text, api_key):
    if not api_key: return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = f"è«‹å°‡å¤ªé­¯é–£èªå¥å­ã€Œ{text}ã€ç¿»è­¯æˆé€šé †çš„ç¹é«”ä¸­æ–‡ï¼Œç›´æ¥çµ¦å‡ºç¿»è­¯çµæœå³å¯ã€‚"
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception:
        return "(ç¿»è­¯å¤±æ•—)"

# ==========================================
# 3. è¼”åŠ©å‡½å¼ï¼šæ–·å¥
# ==========================================
def split_sentences(text):
    # é‡å°åŠå‹æ¨™é»æ–·å¥
    pattern = r'([.?!]+)' 
    parts = re.split(pattern, text)
    sentences = []
    temp_text = ""
    for part in parts:
        if not part: continue 
        if re.match(pattern, part):
            temp_text += part
            sentences.append(temp_text.strip())
            temp_text = "" 
        else:
            temp_text += part
    if temp_text.strip():
        sentences.append(temp_text.strip())
    return sentences

# ==========================================
# ä»‹é¢é‚è¼¯
# ==========================================

st.title("å¤ªé­¯é–£èªæ§‹è©åˆ†æå™¨ (AI Context-Aware)")
st.caption("ğŸš€ èªå¢ƒæ„ŸçŸ¥ç‰ˆï¼šåˆ©ç”¨ AI åˆ†ææ•´å¥ä¸Šä¸‹æ–‡ï¼Œè§£æ±ºå–®å­—æ„æ€ä¸æº–ç¢ºçš„å•é¡Œã€‚")
st.markdown("---")

if "user_input" not in st.session_state:
    st.session_state["user_input"] = ""

def set_example_text(text):
    st.session_state["user_input"] = text

# ç¯„ä¾‹
ex1_text = "Mtalux bi ka hidaw. Mring kana ka hiyi mu."
ex2_text = "Bhangan ka kari o meiyah ka bgihur paru msa."
ex3_text = "Mlatat su o iya bi snguhi madas bubung."

col1, col2 = st.columns([3, 1])
with col1:
    input_text = st.text_area("è«‹è¼¸å…¥å¥å­ (æ—èªæˆ–ä¸­æ–‡)", height=100, 
                              placeholder="æ”¯æ´å¤šå¥è¼¸å…¥ï¼Œä¾‹å¦‚ï¼šMha ku qmita tnkuyan mu.", 
                              key="user_input")
with col2:
    st.write("ç¯„ä¾‹ï¼š")
    st.button("ç¯„ä¾‹ 1", on_click=set_example_text, args=(ex1_text,))
    st.button("ç¯„ä¾‹ 2", on_click=set_example_text, args=(ex2_text,))
    st.button("ç¯„ä¾‹ 3", on_click=set_example_text, args=(ex3_text,))

# ç”¨ä¾†æš«å­˜ AI åˆ†æçµæœ (å¦‚æœæ˜¯å¤šå¥ï¼Œæœ€å¾Œè¦åˆä½µä¸‹è¼‰)
if "analysis_results" not in st.session_state:
    st.session_state["analysis_results"] = []

# åˆ†ææŒ‰éˆ•
if st.button("é–‹å§‹ç²¾æº–åˆ†æ", type="primary"):
    input_content = st.session_state["user_input"]
    
    if not input_content:
        st.warning("è«‹è¼¸å…¥æ–‡å­—")
    else:
        # æ¸…ç©ºèˆŠçµæœ
        st.session_state["analysis_results"] = []
        all_csv_data = [["Line", "Content"]] # CSV Header

        # 1. æ–·å¥
        sentence_list = split_sentences(input_content)
        
        # 2. é€å¥è™•ç†
        for idx, single_sentence in enumerate(sentence_list):
            if len(sentence_list) > 1:
                st.markdown(f"#### å¥å­ {idx + 1}")

            with st.spinner(f"æ­£åœ¨é€²è¡Œèªå¢ƒåˆ†æ... ({idx+1}/{len(sentence_list)})"):
                
                # åˆ¤æ–·æ˜¯å¦ç‚ºä¸­æ–‡ (å¦‚æœæ˜¯ä¸­æ–‡ï¼Œå…ˆç¿»æˆæ—èª)
                is_chinese = any("\u4e00" <= char <= "\u9fff" for char in single_sentence)
                source_sentence = single_sentence
                
                if is_chinese:
                    if not apiKey:
                        st.error("éœ€è¦ API Key æ‰èƒ½è™•ç†ä¸­æ–‡ã€‚")
                        st.stop()
                    # å‘¼å«ç¿»è­¯ API
                    translated = call_ai_translation(f"è«‹å°‡ä¸­æ–‡ '{single_sentence}' ç¿»è­¯æˆå¤ªé­¯é–£èª", apiKey)
                    if translated:
                        # é¡¯ç¤ºç¿»è­¯éç¨‹
                        st.info(f"ä¸­æ–‡ç¿»è­¯åµæ¸¬ï¼š {single_sentence}  â¡ï¸  {translated}")
                        source_sentence = translated
                    else:
                        st.error("ç¿»è­¯å¤±æ•—")
                        st.stop()

                # --- æ ¸å¿ƒï¼šå‘¼å« AI é€²è¡Œæ•´å¥èªå¢ƒåˆ†æ ---
                if apiKey:
                    # é€™æ˜¯æœ€é‡è¦çš„ä¸€æ­¥ï¼šç›´æ¥å• AI é€™å¥è©±æ¯å€‹å­—çš„æ„æ€
                    ai_analysis_json = call_ai_sentence_analysis(source_sentence, apiKey)
                    
                    if isinstance(ai_analysis_json, list):
                        # AI æˆåŠŸå›å‚³ List
                        analyzed_words = ai_analysis_json
                        
                        # é †ä¾¿å–å¾—æ•´å¥ç¿»è­¯ (å¯ä»¥ç›´æ¥ç”¨ä¸Šé¢çš„ï¼Œæˆ–æ˜¯å†å•ä¸€æ¬¡æ›´æº–çš„)
                        full_translation = call_ai_translation(source_sentence, apiKey)
                    else:
                        # AI å›å‚³éŒ¯èª¤
                        st.error(f"AI åˆ†æç™¼ç”ŸéŒ¯èª¤: {ai_analysis_json}")
                        analyzed_words = [{"original": w, "morph": "???", "gloss": "???", "meaning": "???"} for w in source_sentence.split()]
                        full_translation = "(åˆ†æå¤±æ•—)"
                else:
                    # æ²’æœ‰ API Key çš„é™ç´šè™•ç† (åªæŸ¥å­—å…¸)
                    st.error("è«‹è¼¸å…¥ API Key ä»¥ç²å¾—ç²¾æº–èªå¢ƒåˆ†æã€‚ç›®å‰åƒ…é¡¯ç¤ºåŸºç¤å­—å…¸åŒ¹é…ã€‚")
                    analyzed_words = []
                    for w in source_sentence.split():
                        clean_w = re.sub(r'[.,?!]', '', w).lower()
                        d = DICTIONARY.get(clean_w, {"morph": w, "gloss": "???", "meaning": "???"})
                        analyzed_words.append({"original": w, "morph": d["morph"], "gloss": d["gloss"], "meaning": d["meaning"]})
                    full_translation = "(ç„¡ API Key)"

                # --- é¡¯ç¤ºçµæœ (å››è¡Œæ¨™è¨») ---
                # æº–å‚™è³‡æ–™
                line1 = [w.get('original', '') for w in analyzed_words]
                line2 = [w.get('morph', '') for w in analyzed_words]
                line3 = [w.get('gloss', '') for w in analyzed_words]
                # é€™è£¡çš„ Meaning ä¾†è‡ª AI çš„èªå¢ƒåˆ†æï¼Œä¸å†æ˜¯å­—å…¸æ­»æ¿çš„æ„æ€
                line4_words = [w.get('meaning', '') for w in analyzed_words] 
                
                # é¡¯ç¤º HTML
                html_output = f"""
                <div style="font-family: monospace; font-size: 16px; line-height: 1.8; background-color: #f8f9fa; color: #1f2937; padding: 20px; border-radius: 10px; margin-bottom: 20px; overflow-x: auto;">
                    <div style="margin-bottom: 8px; white-space: nowrap;"><span style="color: #e11d48; font-weight: bold; margin-right: 10px;">â— åŸå¥</span> {' '.join(line1)}</div>
                    <div style="margin-bottom: 8px; white-space: nowrap;"><span style="color: #2563eb; font-weight: bold; margin-right: 10px;">â— æ§‹è©</span> {' '.join(line2)}</div>
                    <div style="margin-bottom: 8px; white-space: nowrap;"><span style="color: #059669; font-weight: bold; margin-right: 10px;">â— è©æ³•</span> {' '.join(line3)}</div>
                    <div style="margin-bottom: 8px; white-space: nowrap;"><span style="color: #7c3aed; font-weight: bold; margin-right: 10px;">â— é‡‹ç¾©</span> {' '.join(line4_words)}</div>
                    <div style="margin-top: 12px; padding-top: 8px; border-top: 1px solid #d1d5db; font-weight: bold;">
                        <span style="color: #d97706;">â— æ•´å¥</span> {full_translation}
                    </div>
                </div>
                """
                st.markdown(html_output, unsafe_allow_html=True)

                # æ”¶é›† CSV è³‡æ–™
                all_csv_data.append([f"Sentence {idx+1} - Line 1 (Original)", ' '.join(line1)])
                all_csv_data.append([f"Sentence {idx+1} - Line 2 (Morph)", ' '.join(line2)])
                all_csv_data.append([f"Sentence {idx+1} - Line 3 (Gloss)", ' '.join(line3)])
                all_csv_data.append([f"Sentence {idx+1} - Line 4 (Meaning)", ' '.join(line4_words)]) # é€™æ˜¯å–®å­—æ„æ€
                all_csv_data.append([f"Sentence {idx+1} - Translation", full_translation]) # é€™æ˜¯æ•´å¥ç¿»è­¯
                all_csv_data.append(["---", "---"])

        # 3. åŒ¯å‡ºæŒ‰éˆ•
        df_export = pd.DataFrame(all_csv_data)
        csv = df_export.to_csv(index=False, header=False).encode('utf-8-sig')
        
        st.download_button(
            label="ğŸ“¥ åŒ¯å‡º Excel (CSV)",
            data=csv,
            file_name='truku_smart_analysis.csv',
            mime='text/csv',
        )

st.markdown("---")
st.caption("è³‡æ–™ä¾†æºï¼šAI ä¸Šä¸‹æ–‡èªå¢ƒåˆ†æ (Gemini 2.5) | è¨­è¨ˆç”¨é€”ï¼šæ—èªæ•™å­¸èˆ‡èªæ–™ä¿å­˜")
